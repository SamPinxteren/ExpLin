{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661d1168-bbde-4d7d-9b35-7872b23bff5d",
   "metadata": {},
   "source": [
    "The goal of this experiment is to see how well an explanation correlates with annotated bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d93944-2983-4b61-881b-4157122d6390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "import torch\n",
    "from minmax.lmm_models import vgg, resnet, densenet\n",
    "from torchvision.models import vgg as vggn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_transforms = {\n",
    "    'train': T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'inverse': T.Compose([\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "        T.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
    "    ]),\n",
    "    'target': T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda X: X < 0.007),\n",
    "        T.Lambda(lambda X: X[0,:,:])\n",
    "    ])\n",
    "}\n",
    "\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "oxfordpets = OxfordIIITPet(\n",
    "    root='~/datasets/',\n",
    "    split='trainval',\n",
    "    target_types='segmentation',\n",
    "    transform=data_transforms['train'],\n",
    "    target_transform=data_transforms['target']\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    oxfordpets,\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "netn = vggn.vgg19(pretrained=True)\n",
    "net = vgg.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a6c32-8bc3-4b56-b9d5-2596a2c971ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from minmax.mm_linearize import mmTensor, explainV1, explainV2, explainGradient, prediction_score\n",
    "from sklearn import metrics\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from captum.attr import (\n",
    "    Occlusion,\n",
    "    NoiseTunnel,\n",
    "    DeepLiftShap,\n",
    "    Saliency,\n",
    "    GuidedGradCam\n",
    ")\n",
    "\n",
    "OUTPUT_FOLDER = \"Experiment_4\"\n",
    "try:\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "    print(\"Output Folder Created\")\n",
    "except FileExistsError:\n",
    "    print(\"Output Folder Exists\")\n",
    "\n",
    "\n",
    "def auc(y, pred):\n",
    "    y = y.flatten()\n",
    "    pred = pred.flatten()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "for i, (data, mask) in enumerate(testloader):\n",
    "    R = []\n",
    "    \n",
    "    target = netn(data).argmax().item()\n",
    "    \n",
    "    # Guided GradCam\n",
    "    method = {\n",
    "        'name': 'Guided-GradCam',\n",
    "        'time': datetime.now(),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    timeGuidedGradCam = datetime.now()\n",
    "    expl = GuidedGradCam(netn, netn.features[-1])\n",
    "    explGuidedGradCam = expl.attribute(data, target=target)\n",
    "    explGuidedGradCam = explGuidedGradCam.detach().numpy()\n",
    "    explGuidedGradCam = explGuidedGradCam.sum(axis=0)\n",
    "    explGuidedGradCam = np.linalg.norm(explGuidedGradCam, axis=0)\n",
    "    method['expl'] = explGuidedGradCam\n",
    "    method['score'] = auc(mask, method['expl'])\n",
    "    method['time'] = (datetime.now() - method['time']).seconds\n",
    "    R.append(method)\n",
    "\n",
    "    # SmoothGrad\n",
    "    method = {\n",
    "        'name': 'SmoothGrad',\n",
    "        'time': datetime.now(),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    saliency = Saliency(netn)\n",
    "    nt = NoiseTunnel(saliency)\n",
    "    explSmoothGrad = nt.attribute(data, nt_type='smoothgrad', nt_samples=15, target=target)\n",
    "    explSmoothGrad = explSmoothGrad.detach().numpy().sum(axis=0)\n",
    "    explSmoothGrad = np.linalg.norm(explSmoothGrad, axis=0)\n",
    "    method['expl'] = explSmoothGrad\n",
    "    method['score'] = auc(mask, method['expl'])\n",
    "    method['time'] = (datetime.now() - method['time']).seconds\n",
    "    R.append(method)\n",
    "    \n",
    "    # DeepLiftShap\n",
    "    method = {\n",
    "        'name': 'DeepLiftShap',\n",
    "        'time': datetime.now(),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    dist = torch.cat([data * 0, data * 1])\n",
    "    expl = DeepLiftShap(netn)\n",
    "    explShapley = expl.attribute(data, baselines=dist, target=target)\n",
    "    explShapley = explShapley.detach().numpy().sum(axis=0)\n",
    "    explShapley = np.linalg.norm(explShapley, axis=0)\n",
    "    method['expl'] = explShapley\n",
    "    method['score'] = auc(mask, method['expl'])\n",
    "    method['time'] = (datetime.now() - method['time']).seconds\n",
    "    R.append(method)\n",
    "\n",
    "    # Occlusion\n",
    "    method = {\n",
    "        'name': 'Occlusion',\n",
    "        'time': datetime.now(),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    expl = Occlusion(netn)\n",
    "    explOcclusion = expl.attribute(data, target=target, strides=(3,25,25), sliding_window_shapes=(3,50,50))\n",
    "    explOcclusion = explOcclusion.detach().numpy().sum(axis=0).sum(axis=0)\n",
    "    if not target: explOcclusion = -explOcclusion\n",
    "    explOcclusion -= explOcclusion.min()\n",
    "    explOcclusion /= explOcclusion.max()\n",
    "    method['expl'] = explOcclusion\n",
    "    method['score'] = auc(mask, method['expl'])\n",
    "    method['time'] = (datetime.now() - method['time']).seconds\n",
    "    R.append(method)\n",
    "\n",
    "    # Gradient\n",
    "    method = {\n",
    "        'name': 'Gradient',\n",
    "        'time': datetime.now(),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    explGradient, _ = explainGradient(net, data)\n",
    "    method['expl'] = explGradient\n",
    "    method['score'] = auc(mask, method['expl'])\n",
    "    method['time'] = (datetime.now() - method['time']).seconds\n",
    "    R.append(method)\n",
    "\n",
    "    # RangeGrad\n",
    "    method = {\n",
    "        'name': 'RangeGrad',\n",
    "        'time': datetime.now(),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    explRangeGrad, _ = explainV2(net, data, scale=1/100, width=1/1000)\n",
    "    method['expl'] = explRangeGrad\n",
    "    method['score'] = auc(mask, method['expl'])\n",
    "    method['time'] = (datetime.now() - method['time']).seconds\n",
    "    R.append(method)\n",
    "\n",
    "    # Mask \n",
    "    method = {\n",
    "        'name': 'Mask',\n",
    "        'expl': mask.sum(axis=0),\n",
    "        'cmap': plt.cm.Reds\n",
    "    }\n",
    "    R.append(method)\n",
    "\n",
    "    # Image\n",
    "    method = {\n",
    "        'name': 'Original',\n",
    "        'expl': data_transforms['inverse'](data)[0].movedim(0,2).numpy(),\n",
    "        'cmap': None\n",
    "    }\n",
    "    R.append(method)\n",
    "    \n",
    "    \n",
    "    # Plot all versions of the explanation\n",
    "    fig, axs = plt.subplots(2, 4)\n",
    "    for ax in axs.ravel(): ax.axis('off')\n",
    "    fig.set_size_inches(17,10)\n",
    "    \n",
    "    for j, d in enumerate(R):\n",
    "        p = divmod(j, 4)\n",
    "        axs[p].imshow(d['expl'], cmap=d['cmap'])\n",
    "        if 'score' in d:\n",
    "            axs[p].set_title(\"{name} {score:.3f}\".format(**d))\n",
    "        else:\n",
    "            axs[p].set_title(\"{name}\".format(**d))\n",
    "        filename = \"{dir}/{i}_{name}.png\".format(i=i, dir=OUTPUT_FOLDER, **d)\n",
    "        plt.imsave(filename, d['expl'], cmap=d['cmap'])\n",
    "    plt.show()\n",
    "    \n",
    "    # Add to data table\n",
    "    \"\"\"\n",
    "    with open(\"Experiment_4/scores_trained.txt\", \"a\") as f:\n",
    "        f.write(\"{},{},{},{},{},{}\\n\".format(scoreGuidedGradCam, scoreSmoothGrad, scoreShapley, scoreOcclusion, scoreGradient, scoreRangeGrad))\n",
    "    with open(\"Experiment_4/times_trained.txt\", \"a\") as f:\n",
    "        f.write(\"{},{},{},{},{},{}\\n\".format(timeGuidedGradCam, timeSmoothGrad, timeShapley, timeOcclusion, timeGradient, timeRangeGrad))\n",
    "    \"\"\"\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4feb7-8714-4ef9-a992-e251a48adfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import statistics as stats\n",
    "import scipy.stats as st\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# define the file path and read the file\n",
    "filepath = 'Experiment_4/scores_trained.txt'\n",
    "data = []\n",
    "with open(filepath, 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for row in csvreader:\n",
    "        data.append(row)\n",
    "\n",
    "# define the file path and read the file\n",
    "filepath = 'Experiment_4/times_trained.txt'\n",
    "time_data = []\n",
    "with open(filepath, 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for row in csvreader:\n",
    "        time_data.append(row)\n",
    "\n",
    "# transpose the data to make columns into rows\n",
    "data = list(map(list, zip(*data)))\n",
    "time_data = list(map(list, zip(*time_data)))\n",
    "labels = [\"Guided-GradCam\", \"SmoothGrad\", \"DeepLiftShapley\", \"Occlusion\", \"Gradient\", \"RangeGrad\"]\n",
    "\n",
    "print(\"\\\\begin{tabular}{r|rl|rl}\")\n",
    "print(\"\\tMethod & \\\\multicolumn{2}{c}{AUC} & \\\\multicolumn{2}{c}{Runtime (s)} \\\\\\\\\\\\hline\")\n",
    "for i in range(6):\n",
    "    # calculate mean and confidence interval\n",
    "    d = [float(j) for j in data[i]]\n",
    "    time_d = [float(j) for j in time_data[i]]\n",
    "    mean = stats.mean(d)\n",
    "    ci = t.interval(0.95, len(d)-1, scale=st.sem(d))\n",
    "    time_mean = stats.mean(time_d)\n",
    "    time_ci = t.interval(0.95, len(time_d)-1, scale=st.sem(time_d))\n",
    "    print(\"\\t{} & {:.3f} & \\\\textpm {:.3f} & {:.1f} & \\\\textpm {:.1f} \\\\\\\\\".format(labels[i], mean, ci[1], time_mean, time_ci[1]))\n",
    "print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b46bb-c875-4701-9c00-eb09c84d1600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV3.8",
   "language": "python",
   "name": "env3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
