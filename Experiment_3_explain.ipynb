{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefe1639-182a-4664-9362-365f1626bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained models\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load Data as 4 images in a 2x2 grid\n",
    "\"\"\"\n",
    "from torchvision import transforms as T\n",
    "from random import sample, shuffle\n",
    "import os\n",
    "import torch\n",
    "from minmax.lmm_models import vgg, resnet, densenet\n",
    "from torchvision.models import vgg as vggn\n",
    "import minmax.mm_linearize as mm \n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_transforms = {\n",
    "    'train': T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': T.Compose([\n",
    "        T.Resize((224,224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'inverse': T.Compose([\n",
    "        T.Normalize(mean=[0, 0, 0], std=[1/0.229, 1/0.224, 1/0.225]),\n",
    "        T.Normalize(mean=[-0.485, -0.456, -0.406], std=[1, 1, 1]),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Create ImageNet dataset\n",
    "from torchvision.datasets import ImageNet\n",
    "imagenet = ImageNet(\n",
    "    root='~/datasets/ImageNet',\n",
    "    split=\"val\",\n",
    "    transform=data_transforms['train']\n",
    ")\n",
    "\n",
    "# Create oxford pets binary classification dataset (cats vs dogs)\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "oxfordpets = OxfordIIITPet(\n",
    "    root='~/datasets/',\n",
    "    split='trainval',\n",
    "    transform=data_transforms['train'],\n",
    "    target_transform=lambda x: x+1 in [1, 6, 7, 8, 10, 12, 21, 24, 27, 28, 33, 34]\n",
    ")\n",
    "\n",
    "class QuartileDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Compose 4 images into one image\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_1, dataset_2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_1 (Dataset): The main dataset with target images\n",
    "            dataset_2 (Dataset): The data used to fill the other 3 spots\n",
    "        \"\"\"\n",
    "        self.dataset_1 = dataset_1\n",
    "        self.dataset_2 = dataset_2\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        ##########\n",
    "        # \n",
    "        # \n",
    "        \"\"\"\n",
    "        X1, Y = self.dataset_1[idx]\n",
    "        X2, _ = self.dataset_2[(idx) % len(self.dataset_2)]\n",
    "        X3, _ = self.dataset_2[(2*idx) % len(self.dataset_2)]\n",
    "        X4, _ = self.dataset_2[(3*idx) % len(self.dataset_2)]\n",
    "        corner = idx % 4\n",
    "        if corner == 0:\n",
    "            pass\n",
    "        elif corner == 1:\n",
    "            X1, X2 = X2, X1\n",
    "        elif corner == 2:\n",
    "            X1, X3 = X3, X1\n",
    "        elif corner == 3:\n",
    "            X1, X4 = X4, X1\n",
    "        h1 = torch.cat((X1, X2), dim=2)\n",
    "        h2 = torch.cat((X3, X4), dim=2)\n",
    "        X = torch.cat([h1, h2], dim=1)\n",
    "        return X, (Y, idx % 4)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    QuartileDataset(oxfordpets, imagenet),\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "TRAINED = True\n",
    "\"\"\"\n",
    "Load a pytorch network (vgg19)\n",
    "\"\"\"\n",
    "if TRAINED:\n",
    "    netn = torch.load(\"finetuned.pt\")\n",
    "    net = vgg.vgg19()\n",
    "    net.classifier[6] = mm.Linear(4096, 1)\n",
    "    net.load_state_dict(netn.state_dict())\n",
    "    print(\"Loaded trained models\")\n",
    "else:\n",
    "    netn = vggn.vgg19()\n",
    "    netn.classifier[6] = nn.Linear(4096, 1)\n",
    "    net = vgg.vgg19()\n",
    "    net.classifier[6] = mm.Linear(4096, 1)\n",
    "    print(\"Loaded untrained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29559fa5-2cb9-4314-b5e7-92d52fd61436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Folder Exists\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from minmax.mm_linearize import mmTensor, explainV1, explainV2, explainGradient, prediction_score\n",
    "from sklearn import metrics\n",
    "from pytorch_grad_cam import GradCAM\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from captum.attr import (\n",
    "    Occlusion,\n",
    "    NoiseTunnel,\n",
    "    DeepLiftShap,\n",
    "    Saliency,\n",
    "    GuidedGradCam\n",
    ")\n",
    "\n",
    "def auc(y, pred):\n",
    "    y = y.flatten()\n",
    "    pred = pred.flatten()\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    return metrics.auc(fpr, tpr)\n",
    "\n",
    "OUTPUT_FOLDER = \"Experiment_3\"\n",
    "try:\n",
    "    os.mkdir(OUTPUT_FOLDER)\n",
    "    print(\"Output Folder Created\")\n",
    "except FileExistsError:\n",
    "    print(\"Output Folder Exists\")\n",
    "    \n",
    "for i, (data, (target, corner)) in enumerate(testloader):\n",
    "    # Create correct quartile maks\n",
    "    M1 = torch.ones((224,224))\n",
    "    M2 = torch.zeros((224,224))\n",
    "    M3 = torch.zeros((224,224))\n",
    "    M4 = torch.zeros((224,224))\n",
    "    if corner == 0:\n",
    "        pass\n",
    "    elif corner == 1:\n",
    "        M1, M2 = M2, M1\n",
    "    elif corner == 2:\n",
    "        M1, M3 = M3, M1\n",
    "    elif corner == 3:\n",
    "        M1, M4 = M4, M1\n",
    "    h1 = torch.cat((M1, M2), dim=1)\n",
    "    h2 = torch.cat((M3, M4), dim=1)\n",
    "    mask = torch.cat([h1, h2], dim=0)\n",
    "\n",
    "    # Plot all versions of the explanation\n",
    "    fig, axs = plt.subplots(2, 4)\n",
    "    fig.set_size_inches(17,10)\n",
    "    \n",
    "    score = torch.sigmoid(prediction_score(net, data))\n",
    "\n",
    "    # Guided GradCam\n",
    "    timeGuidedGradCam = datetime.now()\n",
    "    expl = GuidedGradCam(netn, netn.features[-1])\n",
    "    explGuidedGradCam = expl.attribute(data)\n",
    "    explGuidedGradCam = explGuidedGradCam.detach().numpy()\n",
    "    explGuidedGradCam = explGuidedGradCam.sum(axis=0)\n",
    "    explGuidedGradCam = np.linalg.norm(explGuidedGradCam, axis=0)\n",
    "    scoreGuidedGradCam = auc(mask, explGuidedGradCam)\n",
    "    \n",
    "    axs[0][0].axis(\"off\")\n",
    "    axs[0][0].imshow(explGuidedGradCam, cmap=plt.cm.Reds)\n",
    "    axs[0][0].set_title(\"Guided-GradCam {:.3f}\".format(scoreGuidedGradCam))\n",
    "    timeGuidedGradCam = (datetime.now() - timeGuidedGradCam).seconds\n",
    "    if i < 5: plt.imsave(\"{}/{}_guidedgradcam.png\".format(OUTPUT_FOLDER, i), explGuidedGradCam, cmap=plt.cm.Reds)\n",
    "\n",
    "    # SmoothGrad\n",
    "    timeSmoothGrad = datetime.now()\n",
    "    saliency = Saliency(netn)\n",
    "    nt = NoiseTunnel(saliency)\n",
    "    explSmoothGrad = nt.attribute(data, nt_type='smoothgrad', nt_samples=15)\n",
    "    explSmoothGrad = explSmoothGrad.detach().numpy().sum(axis=0)\n",
    "    explSmoothGrad = np.linalg.norm(explSmoothGrad, axis=0)\n",
    "    scoreSmoothGrad = auc(mask, explSmoothGrad)\n",
    "    \n",
    "    axs[0][1].axis(\"off\")\n",
    "    axs[0][1].imshow(explSmoothGrad, cmap=plt.cm.Reds)\n",
    "    axs[0][1].set_title(\"SmoothGrad {:.3f}\".format(scoreSmoothGrad))\n",
    "    timeSmoothGrad = (datetime.now() - timeSmoothGrad).seconds\n",
    "    if i < 5: plt.imsave(\"{}/{}_smoothgrad.png\".format(OUTPUT_FOLDER, i), explSmoothGrad, cmap=plt.cm.Reds)\n",
    "    \n",
    "    # DeepLiftShap\n",
    "    timeShapley = datetime.now()\n",
    "    dist = torch.cat([data * 0, data * 1])\n",
    "    expl = DeepLiftShap(netn)\n",
    "    explShapley = expl.attribute(data, baselines=dist)\n",
    "    explShapley = explShapley.detach().numpy().sum(axis=0)\n",
    "    explShapley = np.linalg.norm(explShapley, axis=0)\n",
    "    scoreShapley = auc(mask, explShapley)\n",
    "    \n",
    "    axs[0][2].axis(\"off\")\n",
    "    axs[0][2].imshow(explShapley, cmap=plt.cm.Reds)\n",
    "    axs[0][2].set_title(\"Shapley {:.3f}\".format(scoreShapley))\n",
    "    timeShapley = (datetime.now() - timeShapley).seconds\n",
    "    if i < 5: plt.imsave(\"{}/{}_shapley.png\".format(OUTPUT_FOLDER, i), explShapley, cmap=plt.cm.Reds)\n",
    "\n",
    "    # Occlusion\n",
    "    timeOcclusion = datetime.now()\n",
    "    expl = Occlusion(netn)\n",
    "    explOcclusion = expl.attribute(data, strides=(3,25,25), sliding_window_shapes=(3,50,50))\n",
    "    explOcclusion = explOcclusion.detach().numpy().sum(axis=0).sum(axis=0)\n",
    "    if not target: explOcclusion = -explOcclusion\n",
    "    explOcclusion -= explOcclusion.min()\n",
    "    explOcclusion /= explOcclusion.max()\n",
    "    scoreOcclusion = auc(mask, explOcclusion)\n",
    "    \n",
    "    axs[0][3].axis(\"off\")\n",
    "    axs[0][3].imshow(explOcclusion, cmap=plt.cm.Reds)\n",
    "    axs[0][3].set_title(\"Occlusion {:.3f}\".format(scoreOcclusion))\n",
    "    timeOcclusion = (datetime.now() - timeOcclusion).seconds\n",
    "    if i < 5: plt.imsave(\"{}/{}_occlusion.png\".format(OUTPUT_FOLDER, i), explOcclusion, cmap=plt.cm.Reds)\n",
    "\n",
    "    # Gradient\n",
    "    timeGradient = datetime.now()\n",
    "    explGradient, _ = explainGradient(net, data)\n",
    "    scoreGradient = auc(mask, explGradient)\n",
    "    \n",
    "    axs[1][0].axis(\"off\")\n",
    "    axs[1][0].imshow(explGradient, cmap=plt.cm.Reds)\n",
    "    axs[1][0].set_title(\"Gradient {:.3f}\".format(scoreGradient))\n",
    "    timeGradient = (datetime.now() - timeGradient).seconds\n",
    "    if i < 5: plt.imsave(\"{}/{}_gradient.png\".format(OUTPUT_FOLDER, i), explGradient, cmap=plt.cm.Reds)\n",
    "\n",
    "    # RangeGrad\n",
    "    timeRangeGrad = datetime.now()\n",
    "    explRangeGrad, _ = explainV2(net, data, scale=1/100, width=1/1000)\n",
    "    scoreRangeGrad = auc(mask, explRangeGrad)\n",
    "    axs[1][1].axis(\"off\")\n",
    "    axs[1][1].imshow(explRangeGrad, cmap=plt.cm.Reds)\n",
    "    axs[1][1].set_title(\"RangeGrad {:.3f}\".format(scoreRangeGrad))\n",
    "    timeRangeGrad = (datetime.now() - timeRangeGrad).seconds\n",
    "    if i < 5: plt.imsave(\"{}/{}_rangegrad.png\".format(OUTPUT_FOLDER, i), explRangeGrad, cmap=plt.cm.Reds)\n",
    "\n",
    "    # Mask \n",
    "    axs[1][2].axis(\"off\")\n",
    "    axs[1][2].imshow(mask, cmap=plt.cm.Reds)\n",
    "    axs[1][2].set_title(\"C: {}\".format(corner.item()))\n",
    "    if i < 5: plt.imsave(\"{}/{}_mask.png\".format(OUTPUT_FOLDER, i), mask, cmap=plt.cm.Reds)\n",
    "\n",
    "    # Image\n",
    "    image = data_transforms['inverse'](data)[0].movedim(0,2).numpy()\n",
    "    axs[1][3].axis(\"off\")\n",
    "    axs[1][3].imshow(image)\n",
    "    axs[1][3].set_title(\"T: {}, P: {}\".format(target.item(), score > 0.5))\n",
    "    plt.show()\n",
    "    if i < 5: plt.imsave(\"{}/{}_original.png\".format(OUTPUT_FOLDER, i), image)\n",
    "    \n",
    "    # Add to data table\n",
    "    with open(\"Experiment_3/scores_finetuned.txt\", \"a\") as f:\n",
    "        f.write(\"{},{},{},{},{},{}\\n\".format(scoreGuidedGradCam, scoreSmoothGrad, scoreShapley, scoreOcclusion, scoreGradient, scoreRangeGrad))\n",
    "    with open(\"Experiment_3/times_finetuned.txt\", \"a\") as f:\n",
    "        f.write(\"{},{},{},{},{},{}\\n\".format(timeGuidedGradCam, timeSmoothGrad, timeShapley, timeOcclusion, timeGradient, timeRangeGrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3191d368-fc92-4e17-aefa-3d2c617f9ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{r|rl|rl}\n",
      "\tMethod & \\multicolumn{2}{c}{AUC} & \\multicolumn{2}{c}{Runtime (s)} \\\\\\hline\n",
      "\tGuided-GradCam & 0.571 & \\textpm 0.012 & 1.1 & \\textpm 0.0 \\\\\n",
      "\tSmoothGrad & 0.590 & \\textpm 0.010 & 17.2 & \\textpm 0.0 \\\\\n",
      "\tDeepLiftShapley & 0.698 & \\textpm 0.008 & 8.1 & \\textpm 0.0 \\\\\n",
      "\tOcclusion & 0.610 & \\textpm 0.006 & 83.7 & \\textpm 0.1 \\\\\n",
      "\tGradient & 0.684 & \\textpm 0.009 & 6.0 & \\textpm 0.0 \\\\\n",
      "\tRangeGrad & 0.663 & \\textpm 0.010 & 10.8 & \\textpm 0.0 \\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import statistics as stats\n",
    "import scipy.stats as st\n",
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# define the file path and read the file\n",
    "filepath = 'Experiment_3/scores_finetuned.txt'\n",
    "data = []\n",
    "with open(filepath, 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for row in csvreader:\n",
    "        data.append(row)\n",
    "\n",
    "# define the file path and read the file\n",
    "filepath = 'Experiment_3/times_finetuned.txt'\n",
    "time_data = []\n",
    "with open(filepath, 'r') as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    for row in csvreader:\n",
    "        time_data.append(row)\n",
    "\n",
    "# transpose the data to make columns into rows\n",
    "data = list(map(list, zip(*data)))\n",
    "time_data = list(map(list, zip(*time_data)))\n",
    "labels = [\"Guided-GradCam\", \"SmoothGrad\", \"DeepLiftShapley\", \"Occlusion\", \"Gradient\", \"RangeGrad\"]\n",
    "\n",
    "print(\"\\\\begin{tabular}{r|rl|rl}\")\n",
    "print(\"\\tMethod & \\\\multicolumn{2}{c}{AUC} & \\\\multicolumn{2}{c}{Runtime (s)} \\\\\\\\\\\\hline\")\n",
    "for i in range(6):\n",
    "    # calculate mean and confidence interval\n",
    "    d = [float(j) for j in data[i]]\n",
    "    time_d = [float(j) for j in time_data[i]]\n",
    "    mean = stats.mean(d)\n",
    "    ci = t.interval(0.95, len(d)-1, scale=st.sem(d))\n",
    "    time_mean = stats.mean(time_d)\n",
    "    time_ci = t.interval(0.95, len(time_d)-1, scale=st.sem(time_d))\n",
    "    print(\"\\t{} & {:.3f} & \\\\textpm {:.3f} & {:.1f} & \\\\textpm {:.1f} \\\\\\\\\".format(labels[i], mean, ci[1], time_mean, time_ci[1]))\n",
    "print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a664ce-a7f6-45e6-affe-3d024216abcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV3.8",
   "language": "python",
   "name": "env3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
